# Lex Bot v2 Environment Variables
# Copy this file to .env and fill in your values

# === REQUIRED ===
# At least one LLM provider is required
GOOGLE_API_KEY=your_google_api_key_here
# OR
OPENAI_API_KEY=your_openai_api_key_here

# === DATABASE ===
DATABASE_URL=postgresql://user:password@host:5432/database


# Web search (at least one recommended - has fallback chain)
TAVILY_API_KEY=your_tavily_key_here
# Fallback search providers (used if Tavily fails/unavailable)
SERPER_API_KEY=your_serper_key_here
GOOGLE_SERP_API_KEY=your_serpapi_key_here

# === OPTIONAL ===
# Web scraping (Firecrawl as fallback for direct scraping)
FIRECRAWL_API_KEY=your_firecrawl_key_here

# === FUTURE: LEGAL EMBEDDINGS ===
VOYAGE_API_KEY=your_voyage_key_here

# === LLM CONFIGURATION ===
# Single model used for both /chat and /chat/reasoning
# /chat/reasoning adds chain-of-thought, same model
# Switch at runtime via POST /config/llm {"model": "gpt-4o"}
LLM_MODEL=gemini-2.5-flash   # Options: gemini-2.5-flash, gemini-2.5-pro, gpt-4o, gpt-4o-mini

# === LANGSMITH OBSERVABILITY ===
LANGSMITH_API_KEY=lsv2_pt_your_key_here
LANGSMITH_PROJECT=lex-bot-v2

# === TOKEN LIMITS ===
MAX_TOKENS_PER_QUERY=50000
MAX_TOKENS_PER_USER_DAILY=500000

# === TIMEOUTS ===
AGENT_TIMEOUT=30
REQUEST_TIMEOUT=90

# === RATE LIMITING ===
RATE_LIMIT_RPM=10
MAX_QUERY_LENGTH=2000

# === EMBEDDING & RERANK ===
EMBED_MODEL=BAAI/bge-m3
RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# === MEMORY ===
MEM0_ENABLED=true
SESSION_CACHE_TTL=30

# === RETENTION (days) ===
CHAT_RETENTION_DAYS=15
MEM0_RETENTION_DAYS=15

# === RATE LIMITING (SCRAPING) ===
SCRAPE_DELAY=2.5

